ğŸ“˜ Personal RAG Assistant (Flask + ChromaDB + Perplexity API)

A fully functional Retrieval-Augmented Generation (RAG) application built with Flask, ChromaDB, SentenceTransformers, and the Perplexity API.
This system ingests personal .txt files, converts them into embeddings, stores them locally in a vector database, and answers user questions using contextual retrieval.

Designed for privacy, modularity, and real-world AI integration.

ğŸš€ Features

ğŸ” Semantic Search over your personal text data

ğŸ§  Embeddings using SentenceTransformers (all-MiniLM-L6-v2)

ğŸ—‚ Local Vector Storage using ChromaDB

ğŸ¤– LLM Answering via Perplexity Sonar model

ğŸ” Privacy First â€” personal data never leaves your machine

ğŸŒ Flask Web Interface for easy interaction

ğŸ“ Automatic loading of all .txt files from /data folder

ğŸ— Project Structure
flask_rag_app/
â”‚
â”œâ”€â”€ app.py                 # Flask server
â”œâ”€â”€ rag.py                 # RAG engine (embedding, retrieval, LLM)
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ .env                   # PERPLEXITY_API_KEY
â”œâ”€â”€ data/                  # Personal text files (user-provided)
â”œâ”€â”€ vectorstore/           # ChromaDB persistent storage
â”œâ”€â”€ templates/
â”‚     â”œâ”€â”€ index.html       # Query input page
â”‚     â””â”€â”€ result.html      # Answer page
â””â”€â”€ static/
      â””â”€â”€ styles.css       # Optional styling

âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone the Repository
git clone https://github.com/YOUR_USERNAME/YOUR_REPO.git
cd YOUR_REPO

2ï¸âƒ£ Create Virtual Environment (Python 3.10)
py -3.10 -m venv venv
venv\Scripts\activate  # Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Add Perplexity API Key

Create a .env file:

PERPLEXITY_API_KEY=your_api_key_here

5ï¸âƒ£ Add Your Personal Text Files

Place all .txt files inside:

data/

6ï¸âƒ£ Run the Application
python app.py


Open in browser:

http://127.0.0.1:5000/

ğŸ”§ How It Works
ğŸ“ 1. Data Ingestion

The app loads all .txt files from /data on startup.

ğŸ§© 2. Text Chunking

Long text is split into smaller meaningful chunks.

ğŸ“ 3. Embedding Generation

Using all-MiniLM-L6-v2 from SentenceTransformers.

ğŸ—„ 4. Vector Storage

Chunks + embeddings + metadata are stored in ChromaDB.

ğŸ” 5. Semantic Retrieval

A user query is embedded â†’ nearest relevant chunks are retrieved.

ğŸ¤– 6. Answer Generation

Chunks + question are sent to Perplexity, which generates the final answer.

ğŸ“¦ requirements.txt
Flask==3.0.3
chromadb==0.5.0
sentence-transformers==2.6.1
transformers==4.40.2
torch==2.2.2
typing_extensions==4.12.1
requests==2.31.0
python-dotenv==1.0.1

ğŸ“Š Result

The app accurately answers questions grounded in personal knowledge.

Retrieval quality is strong due to high-quality embeddings.

Perplexity generates accurate, concise responses.

ğŸ›  Future Improvements

PDF and image OCR ingestion

Chat history system

Multi-user authentication

Deployment on Render/HuggingFace Spaces

Streamlit UI version

ğŸ§‘â€ğŸ’» Author

Sandeep Kumar
AI & Software Enthusiast
Open to collaborations and new projects ğŸš€
